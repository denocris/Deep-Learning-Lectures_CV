{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "meetup_denoising_variational_autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denocris/Deep-Learning-Lectures_CV/blob/master/meetup_denoising_variational_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fz1FL9F5D1t",
        "colab_type": "text"
      },
      "source": [
        "<h1><center> Denoising Variational Autoencoder with TF 2.0 </center></h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qy4qOigfQd_",
        "colab_type": "text"
      },
      "source": [
        "  \n",
        "  <center>  <img src=  https://drive.google.com/uc?id=1NizfTetPzdNakOwyTuA2y-MQYnZEqNYU \" width=\"700\">  </center> \n",
        "  <center>  <img src=https://drive.google.com/uc?id=1A6QMi7EGwnyeee8DsD7UJ0V_ssWyCNNE \" width=\"700\">  </center> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGbc-ia5FFW0",
        "colab_type": "text"
      },
      "source": [
        "This notebook con be found [here](https://github.com/denocris/MHPC-DeepLearning-Lectures) on my  GitHub. I suggest you to open it in Google Colab! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p19lrvbvvYs-",
        "colab_type": "text"
      },
      "source": [
        "### Some Refs\n",
        "\n",
        "* [What is a Variational Autoencoder?](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)\n",
        "\n",
        "* [Variational Autoencoder by Jonathan Hui](https://jhui.github.io/2017/03/06/Variational-autoencoders/)\n",
        "\n",
        "* [Variational autoencoders by Jeremy Jordan](https://www.jeremyjordan.me/variational-autoencoders/)\n",
        "\n",
        "* Original Papers: [Kingma et al.](https://arxiv.org/abs/1312.6114) and [Rezende et al.](https://arxiv.org/abs/1401.4082)\n",
        "\n",
        "\n",
        "### What are we going to learn?\n",
        "\n",
        "* Working on Google Colaboratory\n",
        "\n",
        "* First time ever using the new TensorFlow 2.0\n",
        "\n",
        "* Important [Information Theory](https://www.amazon.com/Elements-Information-Theory-Telecommunications-Processing/dp/0471241954) Concepts \n",
        "\n",
        "* How to build a Denoising VAE in TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGnyIv25UfEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jzB82LX5C3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ53eL54UUib",
        "colab_type": "code",
        "outputId": "4d0bf1bd-f9f9-42ee-f192-ff40d2b08213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-alpha0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuCO5ftpX6-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math as m\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdGoqFhn9j2l",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/file/d/1Y52T3Z4dwRU4Rq5L5bEVYh0d3kU0aVB8/view?usp=sharing\" alt=\"\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtrqdhkMID39",
        "colab_type": "text"
      },
      "source": [
        "## <h1><center>  Information Theory Background \n",
        "\n",
        "\n",
        "### Self-Information\n",
        "\n",
        "The idea behind self-information is the following\n",
        "\n",
        "* if an event always occurs, we associate it with a smaller amount of information. It will not suprise us!\n",
        "* On the other side, a rare event is associated with a huge amount of information. It will suprise us!\n",
        "\n",
        "I am not surprise to see the sunrise every morning (likely event). Instead,  I would be really suprised if tomorrow the Sun will not rise (unlikely event). This amount of surprise or self-information of the event $x$ is quantified by\n",
        "\n",
        "$$I(x) = - \\log p(x),$$\n",
        "\n",
        "where $p(x)$ is the probability of the event $x$. If $p(x)=1$, then self-info is zero. A rare event instead has a huge surpise factor.\n",
        "\n",
        "### Shannon Entropy \n",
        "\n",
        "In terms of self-info, Shannon Entropy is the average self-information (expected value) over all possible values of X.\n",
        "The entropy for a probability $p(x)$ distribution is\n",
        "\n",
        "$$ S = - \\sum_i p(x_i) \\log p(x_i),$$\n",
        "\n",
        "where we assume we know the probability $p$ for each outcome $i$. If we use $log_2$  for our calculation we can interpret entropy as *the minimum number of bits it would take us to encode our information*.\n",
        "\n",
        "For continous variables, we can use the integral form\n",
        "\n",
        "$$ S = - \\int  p(x) \\log p(x) \\, dx,$$\n",
        "\n",
        "where now $p(x)$ is taking the role of a probability density function (PDF). Take in mind that a broad probability density has higher entropy than a narrowed one (think about Gaussian distribution vs delta Dirac, which has $S=0$).\n",
        "\n",
        "In both discrete and continous formulation, we are computing the expectation (i.e. average) of the negative log-probability (i.e. self-info) which is the theoretical minimum encoding size of the information from the event $x$. The same formula is usually written as\n",
        "\n",
        "$$S = \\mathbb E _{\\, x \\sim p} \\left[ -\\log p(x) \\right],$$\n",
        "\n",
        "where $x \\sim p$ means that we calculate the expectation with the probability distribution $p$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BEow_oxYSje",
        "colab_type": "text"
      },
      "source": [
        "Let's give an example! \n",
        "\n",
        "<!---\n",
        "  REMIND to change open with uc\n",
        " https://drive.google.com/open?id=1Y52T3Z4dwRU4Rq5L5bEVYh0d3kU0aVB8\n",
        "--->\n",
        "\n",
        "  <center>  <img src=https://drive.google.com/uc?id=1GaAeK8xIZCVDRb-oHQNUzRuoOprFh1eS \" width=\"700\">  </center> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5E4znWyYWDT",
        "colab_type": "text"
      },
      "source": [
        "Let's say we have to pass a message about what drink Cristiano would take during an event. In general, Cristiano loves [Midori Sour](https://drizly.com/midori-sour/r-b972d5282bec6fe8) , Daiquiri, Spritz and Wine.\n",
        "\n",
        "On Monday, Cristiano loves to listen Jazz and the probability distribution of his choice is: \n",
        "\n",
        "$$P(\\text Midori ) =  P(\\text Daiquiri ) = P(\\text Spritz ) = P(\\text Wine ) = 0.25,$$\n",
        "\n",
        "while the corresponding entropy\n",
        "\n",
        "$$S = - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{4} \\log \\frac{1}{4} = 2$$\n",
        "\n",
        "On Wednesday, he usually meets with some friends after work: \n",
        "\n",
        "$$P(\\text Midori ) = 0.125,\\;  P(\\text Daiquiri ) =0.125,\\;  P(\\text Spritz ) = 0.5,\\; P(\\text  Wine ) = 0.25,$$\n",
        "\n",
        "while the corresponding entropy\n",
        "\n",
        "$$S = - \\frac{1}{8} \\log \\frac{1}{8} - \\frac{1}{8} \\log \\frac{1}{8} - \\frac{1}{2} \\log \\frac{1}{2} - \\frac{1}{4} \\log \\frac{1}{4} = 1.75$$\n",
        "\n",
        "\n",
        "On Thursday, he often goes to an event where cocktail attire dress code is required\n",
        "\n",
        "$$P(\\text Midori ) = 0.95,\\;  P(\\text Daiquiri ) =0.02,\\;  P(\\text Spritz ) = 0.018,\\; P(\\text  Wine ) = 0.012,$$\n",
        "\n",
        "and the corresponding entropy\n",
        "\n",
        "$$S = - 0.95 \\log 0.95 - 0.02 \\log 0.02 - 0.018 \\log 0.018 - 0.012 \\log 0.012 = 0.364$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KReLhZDvICt1",
        "colab_type": "code",
        "outputId": "c85c896c-7150-4921-946f-e6f66a64984b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# On Monday, all drinks have equal probability to be chose\n",
        "entropy_1 = -0.25*m.log(0.25,2)-0.25*m.log(0.25,2)-0.25*m.log(0.25,2)-0.25*m.log(0.25,2)\n",
        "print('On Monday, high entropy: ', entropy_1)\n",
        "\n",
        "# On Wednesday, some are more probable than others\n",
        "entropy_2 = -0.5*m.log(0.5,2)-0.25*m.log(0.25,2)-0.125*m.log(0.125,2)-0.125*m.log(0.125,2)\n",
        "print('On Wednesday, medium entropy: ', entropy_2)\n",
        "\n",
        "# On Thursday, one drink is by far the most probable\n",
        "entropy_3 = -0.95*m.log(0.95,2)-0.02*m.log(0.02,2)-0.018*m.log(0.018,2)-0.012*m.log(0.012,2)\n",
        "print('On Thursday, low entropy: ', entropy_3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On Monday, high entropy:  2.0\n",
            "On Wednesday, medium entropy:  1.75\n",
            "On Thursday, low entropy:  0.36407300467232967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZh-58l2Qwvu",
        "colab_type": "text"
      },
      "source": [
        "If entropy is high (encoding size $log_2 p(x)$ is big on average), it means we have many message types with small and almost equal probabilities. Hence, every time a new message arrives, you would expect a different type than previous messages. You may see it as a disorder or uncertainty or unpredictability.\n",
        "\n",
        "On the contrary, when a message has much smaller probability than other message, it appears as a surprise because on average you would expect other more frequently sent message types. Moreover, a rare message type has more information than more frequent message types because it eliminates a lot of other probabilities and tells us more specific information.\n",
        "\n",
        "In the drink scenario, by sending “Wine” on thursday which happens 1.2% of the times, we are reducing the uncertainty by 98.8% of the probability distribution (“Midori, Daiquiri, Spritz”) provided we had no information before. If we were sending “Midori” (95%) instead, we would be reducing the uncertainty by 5% only.\n",
        "\n",
        "If the entropy is high (ex: fair coin), the average encoding size is significant which means each message tends to have more (specific) information. Again, this is why high entropy is associated with disorder, uncertainty, surprise, unpredictability, amount of information. The more random a message is, the more information will be gained from decoding the message.\n",
        "\n",
        "Low entropy (ex: sunrise) means that most of the times we are receiving the more predictable information which means less disorder, less uncertainty, less surprise, more predictability and less (specific) information. This is the Thursday case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu6ZmWp5UGDa",
        "colab_type": "text"
      },
      "source": [
        "### Cross Entropy\n",
        "\n",
        "Suppose to have two distributions, the true one $p(x)$ and the estimated $q(x)$. In the language of neural networks, $p(x)$ would be the grond truth (labels in one hot-encoding) and $q(x)$ the outcome of the net, i.e. the one that your machine learning algorithm is trying to match. Cross entropy is a mathematical tool for comparing two probability distributions $p(x)$ and $q(x)$ and it is expressed by the formula \n",
        "\n",
        "$$ H (p,q) = - \\int p(x) \\log q(x)\\,dx.$$\n",
        "\n",
        "If $\\log$ is in base $2$, then cross entropy measures the number of bits you will need encoding symbols from $p$ using the wrong distribution $q$. Subtracting to cross entropy the entropy of $p$, you are counting the cost in terms of bits of using the wrong distribution $q$ (this somehow will be KL-divergence). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60tieif3uc-l",
        "colab_type": "text"
      },
      "source": [
        "### Kullback-Leibler Divergence\n",
        "\n",
        "KL-divergence is just a slight modification of our formula for entropy. Rather than just having our probability distribution $h$ we add into the game our approximating distribution $g$. Then we look at the difference of the log values for each\n",
        "\n",
        "$$D_{KL}(h || g) = - \\sum_i h(x_i) (\\log h(x) - \\log g(x)) = - \\sum_i h(x_i) \\log \\frac{h(x)}{g(x)}$$ \n",
        "\n",
        "KL-divergence is the expectation of the log-difference between the probability of data in the original distribution $h$ with the approximating distribution $g$. Again, if we think in terms of $\\log_2$ we can interpret this as how many bits of information we expect to lose when we choose an approximation $g$ of our original ditribution $h$. \n",
        "\n",
        "In the variational autoencoder loss function, the KL-divergence is used to force the distribution of latent variables $q(z | x)$ to be a normal distribution $n(z)$ so that we can sample latent variables from the normal distribution. As such, the KL-divergence is included in the loss function to improve the similarity between the distribution of latent variables and the normal distribution. More about **KL** can be found [here](https://towardsdatascience.com/demystifying-kl-divergence-7ebe4317ee68) and about **cross-entropy** [here](https://towardsdatascience.com/demystifying-cross-entropy-e80e3ad54a8)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9unYqYhn3bQ",
        "colab_type": "text"
      },
      "source": [
        "##  Autoencoder vs Variational Autoencoder\n",
        "\n",
        "<center>  \n",
        "<p>      \n",
        "<img src=https://drive.google.com/uc?id=1QdiSESYkbNnigrbH-GutlGtRuC6QbGpw  width=\"800\" height=\"250\">  \n",
        "<img src=https://drive.google.com/uc?id=1IiuzC8YeZ_MXJsYg1BgJV-o3VMoQUGvI width=\"800\" height=\"250\">    \n",
        " <p>\n",
        "</center> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTdh_cy0vCgI",
        "colab_type": "text"
      },
      "source": [
        "## <h1><center> Coding our Denoising VAE\n",
        "\n",
        "### Importing Data and Add some Gaussian Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAo8-jgvXp5X",
        "colab_type": "code",
        "outputId": "9c27d079-028d-4178-98bb-a198f5ed4ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\n",
        "\n",
        "\n",
        "# Normalizing the images to the range of [0., 1.]\n",
        "train_images /= 255.\n",
        "test_images /= 255.\n",
        "\n",
        "\n",
        "print(\"train_images.shape: \", train_images.shape)\n",
        "print(\"test_images.shape: \", test_images.shape)\n",
        "\n",
        "noise_variance = 0.2\n",
        "train_images_noisy = train_images + np.random.normal(0, noise_variance, (train_images.shape[0], train_images.shape[1],train_images.shape[2],train_images.shape[3])).astype('float32')\n",
        "test_images_noisy = test_images + np.random.normal(0, noise_variance, (test_images.shape[0], test_images.shape[1],test_images.shape[2], test_images.shape[3])).astype('float32')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_images.shape:  (60000, 28, 28, 1)\n",
            "test_images.shape:  (10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5eUWp3puiQy",
        "colab_type": "code",
        "outputId": "8704bef6-dfc5-4394-aad0-53c811a715a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "image = train_images[0]\n",
        "image = np.array(image, dtype='float')\n",
        "pixels = image.reshape((28, 28))\n",
        "plt.imshow(pixels, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VVekOGyuvC8",
        "colab_type": "code",
        "outputId": "d40447b3-8ad1-448c-cbe6-2d534877c5be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "noisy_image = train_images_noisy[0]\n",
        "noisy_image = np.array(noisy_image, dtype='float')\n",
        "pixels = noisy_image.reshape((28, 28))\n",
        "plt.imshow(pixels, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUiwN-nKvTgu",
        "colab_type": "text"
      },
      "source": [
        "### Transforming Data in TF Dataset Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giJ1nXn6lWFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).batch(BATCH_SIZE)\n",
        "train_dataset_noisy = tf.data.Dataset.from_tensor_slices(train_images_noisy).batch(BATCH_SIZE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(test_images).batch(BATCH_SIZE)\n",
        "test_dataset_noisy = tf.data.Dataset.from_tensor_slices(test_images_noisy).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRFj2SCq0fz9",
        "colab_type": "text"
      },
      "source": [
        "### Using Class to define a Model (From TensorFlow Official Page)\n",
        "\n",
        "   \n",
        "```python\n",
        "class MyModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
        "    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    return self.dense2(x)\n",
        "\n",
        "model = MyModel()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i776XcCtxdKe",
        "colab_type": "text"
      },
      "source": [
        "### Defining our Variational Autoencoder Model\n",
        "<center>  <img src=https://drive.google.com/uc?id=14dNERtYBOiyEq3cPOUFObUe6Ul4cn_r0  \" width=\"800\">  </center> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHt8wvBDl-1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class VAutoEncoder(tf.keras.Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    \n",
        "    super(VAutoEncoder, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    \n",
        "    # Encoder\n",
        "    self.encoder_net = tf.keras.Sequential(\n",
        "      [\n",
        "          tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
        "          tf.keras.layers.Conv2D(\n",
        "              filters=3, kernel_size=10, strides=(2, 2), padding=\"SAME\", activation=tf.nn.relu),\n",
        "          tf.keras.layers.Conv2D(\n",
        "              filters=3, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation=tf.nn.relu),\n",
        "          tf.keras.layers.Flatten(),\n",
        "          # No activation\n",
        "          tf.keras.layers.Dense(latent_dim + latent_dim),\n",
        "      ]\n",
        "    )\n",
        "\n",
        "    # Decoder\n",
        "    self.decoder_net = tf.keras.Sequential(\n",
        "        [\n",
        "          tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "          tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
        "          tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
        "          tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=3, strides=(2, 2), \n",
        "              padding=\"SAME\",\n",
        "              activation=tf.nn.relu),\n",
        "          tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=10, strides=(2, 2),\n",
        "              padding=\"SAME\",\n",
        "              activation=tf.nn.relu),\n",
        "          \n",
        "          # No activation\n",
        "          tf.keras.layers.Conv2DTranspose(\n",
        "              filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  def sample(self, eps=None):\n",
        "    if eps is None:\n",
        "      eps = tf.random.normal(shape=(100, self.latent_dim))\n",
        "    return self.decode(eps, apply_sigmoid=True)\n",
        "\n",
        "  def encode(self, x):\n",
        "    mean, logvar = tf.split(self.encoder_net(x), num_or_size_splits=2, axis=1)\n",
        "    return mean, logvar\n",
        "\n",
        "  def reparameterize(self, mean, logvar):\n",
        "    eps = tf.random.normal(shape=mean.shape)\n",
        "    return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "  def decode(self, z, apply_sigmoid=False):\n",
        "    logits = self.decoder_net(z)\n",
        "    if apply_sigmoid:\n",
        "      probs = tf.sigmoid(logits)\n",
        "      return probs\n",
        "\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA6194-1XLsy",
        "colab_type": "text"
      },
      "source": [
        "### Gaussian Distribution and Loss Function\n",
        "\n",
        "The Gaussian distribution is defined by\n",
        "\n",
        "$$ N (x\\mid \\mu ,\\sigma ^{2})={\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}e^{-{\\frac {(x-\\mu )^{2}}{2\\sigma ^{2}}}} .$$\n",
        "\n",
        "\n",
        "Taking its log we obtain\n",
        "\n",
        "$$ \\log N (x\\mid \\mu ,\\sigma ^{2})= - \\frac{1}{2} \\left( \\log2\\pi  + \\log \\sigma ^2  +  \\frac {(x-\\mu )^{2}}{\\sigma ^{2}}\\right) = - \\frac{1}{2} \\left( \\log2\\pi  + \\log\\sigma ^2 +  (x-\\mu )^{2} \\exp (- \\log\\sigma^2)\\right).$$\n",
        "\n",
        "The loss function of a VAE is\n",
        "\n",
        "$$l_i(\\theta, \\phi) = - \\mathbb{E}_{\\,z\\sim q_{\\theta}(z | x_i)} \\left[ \\log p_{\\phi}(x_i|z) \\right] + \\mathbb K \\mathbb L ( q_{\\theta}(x_i | z) \\, || \\, n(z) ),$$ \n",
        "\n",
        "where the first term is the reconstruction loss, or expected negative log-likelihood of the $i$-th datapoint, while the second is the KL-divergence. The former encourages the decoder to learn to reconstruct the data.The latter is a regularizer that forces the encoder’s distribution to be as much as possible similar to a Gaussian distribution.The final total loss function is $L =\\frac{1}{N} \\sum_i l_i$.\n",
        "\n",
        "  \n",
        "  <center>  <img src=https://drive.google.com/uc?id=1_UmDN6a2I2bktY4nVgT1N2uC-pFxoXo_ \" width=\"800\">  </center> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHdnFL_wGeAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "    log2pi = tf.math.log(2. * np.pi)\n",
        "    return tf.reduce_sum(\n",
        "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "        axis=raxis)\n",
        "\n",
        "def compute_loss(model, x_input, x_target):\n",
        "    mean, logvar = model.encode(x_input)\n",
        "    # This is the key ingredient!\n",
        "    z = model.reparameterize(mean, logvar)\n",
        "    x_logit = model.decode(z)\n",
        "\n",
        "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x_target)\n",
        "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
        "    logpz = log_normal_pdf(z, 0., 0.)\n",
        "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
        "\n",
        "def compute_gradients(model, x_input, x_target):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = compute_loss(model, x_input, x_target)\n",
        "    return tape.gradient(loss, model.trainable_variables), loss\n",
        "\n",
        "def apply_gradients(optimizer, gradients, variables):\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww8drbMT1enI",
        "colab_type": "text"
      },
      "source": [
        "### Defining Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guH1kqSH44is",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10\n",
        "latent_dim = 50\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# keeping the random vector constant for generation (prediction) so\n",
        "# it will be easier to see the improvement.\n",
        "random_vector_for_generation = tf.random.normal(\n",
        "    shape=[num_examples_to_generate, latent_dim])\n",
        "\n",
        "\n",
        "model = VAutoEncoder(latent_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFO3S-AZ0L7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#random_vector_for_generation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hIAa0YA5KWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  predictions = model.sample(test_input)\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()\n",
        "  \n",
        "def original_image(model, images):\n",
        "  \n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(images.shape[0]):\n",
        "      plt.subplot(2, 2, i+1)\n",
        "      plt.imshow(images[i, :, :, 0], cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()\n",
        "\n",
        "  \n",
        "def clean_images(model, test_input):\n",
        "  \n",
        "  mean, logvar = model.encode(test_input)\n",
        "  z = model.reparameterize(mean, logvar)\n",
        "  predictions = model.decode(z, apply_sigmoid=True)\n",
        "  #predictions = model.sample(test_input)\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(2, 2, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX7uZtM65Qv6",
        "colab_type": "code",
        "outputId": "5cb5748d-e50a-450c-d928-7e5ffb1f04cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#generate_and_save_images(model, 0, random_vector_for_generation)\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  start_time = time.time()\n",
        "  for train_x_noisy, train_x in zip(train_dataset_noisy, train_dataset):\n",
        "    gradients, loss = compute_gradients(model, train_x_noisy, train_x)\n",
        "    apply_gradients(optimizer, gradients, model.trainable_variables)\n",
        "  end_time = time.time()\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    loss = tf.keras.metrics.Mean()\n",
        "    for test_x_noisy, test_x in zip(test_dataset_noisy, test_dataset):\n",
        "      loss(compute_loss(model, test_x_noisy,  test_x))\n",
        "    elbo = -loss.result()\n",
        "    display.clear_output(wait=False)\n",
        "    print('Epoch: {}, Test set ELBO: {}, '\n",
        "          'time elapse for current epoch {}'.format(epoch,\n",
        "                                                    elbo,\n",
        "                                                    end_time - start_time))\n",
        "#    generate_and_save_images(\n",
        "#        model, epoch, random_vector_for_generation)\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10, Test set ELBO: -152.4878692626953, time elapse for current epoch 13.718701362609863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqgCV4edrkYi",
        "colab_type": "code",
        "outputId": "d7c05342-c46e-4158-d961-316e522cc06c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "test_dataset_to_be_cleaned = tf.data.Dataset.from_tensor_slices(test_images_noisy).batch(4)\n",
        "\n",
        "\n",
        "cnt = 0\n",
        "\n",
        "for images in test_dataset_to_be_cleaned:\n",
        "  #print(image.shape)\n",
        "  original_image(model, images)\n",
        "  clean_images(model, images)\n",
        "  time.sleep(5)\n",
        "  display.clear_output(wait=False)\n",
        "  cnt += 1\n",
        "  if cnt > 10:\n",
        "    sys.exit()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gaiT2Ru7sUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}